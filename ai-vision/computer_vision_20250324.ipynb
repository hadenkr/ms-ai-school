{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import requests\n",
    "import gradio as gr\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.config import get_config_info\n",
    "from common.image import random_color, get_font"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config_info('./config/settings1.json')\n",
    "\n",
    "FEATURES = ['read', 'smartCrops', 'tags', 'people', 'caption', 'denseCaptions', 'objects']\n",
    "ENDPOINT = config['cv_endpoint']\n",
    "MODEL = config['cv_api_model']\n",
    "API_KEY = config['cv_api_key']\n",
    "API_VERSION = config['cv_api_version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['people', 'tags']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 코드\n",
    "KO_FEATURES = ['read', 'smartCrops', 'tags', 'people']\n",
    "selected_feature_list = ['tags', 'people', 'caption', 'denseCaptions']\n",
    "list(set(KO_FEATURES) & set(selected_feature_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_vision(features, image_path, language, smart_crops=''):\n",
    "    query_params = {\n",
    "        'api-version': API_VERSION,\n",
    "        'features': ','.join(features),\n",
    "        'language': language\n",
    "    }\n",
    "\n",
    "    if 'smartCrops' in  features:\n",
    "        query_params.update({\n",
    "            'smartcrops-aspect-ratios': smart_crops\n",
    "        })\n",
    "    \n",
    "    endpoint = f'{ENDPOINT}/{MODEL}'\n",
    "    # method\n",
    "    headers = {\n",
    "        'Ocp-Apim-Subscription-Key': API_KEY,\n",
    "        'Content-Type': 'application/octet-stream'\n",
    "    }\n",
    "\n",
    "    # image_path 로 이미지를 바이너리 형태로 읽어서 전송한다.\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        image_data = image_file.read()\n",
    "    \n",
    "    response = requests.post(endpoint, params=query_params, headers=headers, data=image_data)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modelVersion': '2023-10-01',\n",
       " 'captionResult': {'text': 'a person using a laptop',\n",
       "  'confidence': 0.8266631960868835},\n",
       " 'denseCaptionsResult': {'values': [{'text': 'a person using a laptop',\n",
       "    'confidence': 0.8266631960868835,\n",
       "    'boundingBox': {'x': 0, 'y': 0, 'w': 1260, 'h': 473}},\n",
       "   {'text': 'a person typing on a laptop',\n",
       "    'confidence': 0.7818875312805176,\n",
       "    'boundingBox': {'x': 450, 'y': 209, 'w': 360, 'h': 239}},\n",
       "   {'text': 'a person holding a cup',\n",
       "    'confidence': 0.7930779457092285,\n",
       "    'boundingBox': {'x': 813, 'y': 303, 'w': 92, 'h': 79}},\n",
       "   {'text': 'a woman sitting at a table using a tablet and a cup',\n",
       "    'confidence': 0.7753849029541016,\n",
       "    'boundingBox': {'x': 648, 'y': 0, 'w': 583, 'h': 455}},\n",
       "   {'text': 'a close up of a cell phone',\n",
       "    'confidence': 0.8052007555961609,\n",
       "    'boundingBox': {'x': 336, 'y': 383, 'w': 129, 'h': 56}},\n",
       "   {'text': 'a blurry picture of a radio',\n",
       "    'confidence': 0.715515673160553,\n",
       "    'boundingBox': {'x': 301, 'y': 48, 'w': 121, 'h': 84}},\n",
       "   {'text': 'a blurry image of a pot on a stove',\n",
       "    'confidence': 0.7917684316635132,\n",
       "    'boundingBox': {'x': 724, 'y': 51, 'w': 151, 'h': 101}},\n",
       "   {'text': 'a computer screen with a screen showing a photo',\n",
       "    'confidence': 0.5710918307304382,\n",
       "    'boundingBox': {'x': 486, 'y': 230, 'w': 197, 'h': 147}},\n",
       "   {'text': 'a blurry image of a blue and white striped object',\n",
       "    'confidence': 0.6781076192855835,\n",
       "    'boundingBox': {'x': 510, 'y': 103, 'w': 213, 'h': 51}},\n",
       "   {'text': 'a person holding a cup',\n",
       "    'confidence': 0.7332363724708557,\n",
       "    'boundingBox': {'x': 816, 'y': 149, 'w': 425, 'h': 317}}]},\n",
       " 'metadata': {'width': 1260, 'height': 473},\n",
       " 'tagsResult': {'values': [{'name': 'computer',\n",
       "    'confidence': 0.9865932464599609},\n",
       "   {'name': 'clothing', 'confidence': 0.9695653915405273},\n",
       "   {'name': 'laptop', 'confidence': 0.9658199548721313},\n",
       "   {'name': 'person', 'confidence': 0.953628659248352},\n",
       "   {'name': 'indoor', 'confidence': 0.9420195817947388},\n",
       "   {'name': 'wall', 'confidence': 0.8871881365776062},\n",
       "   {'name': 'woman', 'confidence': 0.8632701635360718},\n",
       "   {'name': 'using', 'confidence': 0.560353696346283}]},\n",
       " 'objectsResult': {'values': [{'boundingBox': {'x': 730,\n",
       "     'y': 66,\n",
       "     'w': 135,\n",
       "     'h': 85},\n",
       "    'tags': [{'name': 'kitchen appliance', 'confidence': 0.501}]},\n",
       "   {'boundingBox': {'x': 523, 'y': 377, 'w': 185, 'h': 46},\n",
       "    'tags': [{'name': 'computer keyboard', 'confidence': 0.51}]},\n",
       "   {'boundingBox': {'x': 471, 'y': 218, 'w': 289, 'h': 226},\n",
       "    'tags': [{'name': 'Laptop', 'confidence': 0.85}]},\n",
       "   {'boundingBox': {'x': 654, 'y': 0, 'w': 584, 'h': 473},\n",
       "    'tags': [{'name': 'person', 'confidence': 0.855}]}]},\n",
       " 'readResult': {'blocks': [{'lines': [{'text': '............',\n",
       "      'boundingPolygon': [{'x': 628, 'y': 359},\n",
       "       {'x': 681, 'y': 350},\n",
       "       {'x': 682, 'y': 356},\n",
       "       {'x': 630, 'y': 366}],\n",
       "      'words': [{'text': '............',\n",
       "        'boundingPolygon': [{'x': 632, 'y': 359},\n",
       "         {'x': 681, 'y': 351},\n",
       "         {'x': 682, 'y': 356},\n",
       "         {'x': 633, 'y': 366}],\n",
       "        'confidence': 0.707}]}]}]},\n",
       " 'smartCropsResult': {'values': [{'aspectRatio': 0.75,\n",
       "    'boundingBox': {'x': 124, 'y': 0, 'w': 353, 'h': 471}},\n",
       "   {'aspectRatio': 1.8,\n",
       "    'boundingBox': {'x': 112, 'y': 0, 'w': 850, 'h': 471}}]},\n",
       " 'peopleResult': {'values': [{'boundingBox': {'x': 660,\n",
       "     'y': 0,\n",
       "     'w': 584,\n",
       "     'h': 471},\n",
       "    'confidence': 0.9698998332023621},\n",
       "   {'boundingBox': {'x': 566, 'y': 276, 'w': 24, 'h': 30},\n",
       "    'confidence': 0.022009700536727905},\n",
       "   {'boundingBox': {'x': 587, 'y': 273, 'w': 20, 'h': 28},\n",
       "    'confidence': 0.01859394833445549},\n",
       "   {'boundingBox': {'x': 609, 'y': 271, 'w': 19, 'h': 30},\n",
       "    'confidence': 0.003902678843587637},\n",
       "   {'boundingBox': {'x': 563, 'y': 279, 'w': 15, 'h': 28},\n",
       "    'confidence': 0.0034854013938456774},\n",
       "   {'boundingBox': {'x': 566, 'y': 299, 'w': 22, 'h': 41},\n",
       "    'confidence': 0.0031260766554623842},\n",
       "   {'boundingBox': {'x': 570, 'y': 311, 'w': 29, 'h': 38},\n",
       "    'confidence': 0.0026493810582906008},\n",
       "   {'boundingBox': {'x': 588, 'y': 275, 'w': 24, 'h': 54},\n",
       "    'confidence': 0.001754675293341279},\n",
       "   {'boundingBox': {'x': 574, 'y': 274, 'w': 53, 'h': 64},\n",
       "    'confidence': 0.0012078586732968688},\n",
       "   {'boundingBox': {'x': 608, 'y': 270, 'w': 32, 'h': 59},\n",
       "    'confidence': 0.0011869356967508793},\n",
       "   {'boundingBox': {'x': 591, 'y': 305, 'w': 29, 'h': 42},\n",
       "    'confidence': 0.0010676260571926832}]}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response1 = request_vision(FEATURES, './data/windows-kitchen.jpg', 'en', '0.75,1.8')\n",
    "response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_image(image_path, features, response_json):\n",
    "    image = Image.open(image_path)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = get_font()\n",
    "\n",
    "    # boundingBox 그려주는 코드가 추가되어야 함\n",
    "    for feature in features:\n",
    "        name = '{}Result'.format(feature)\n",
    "        # print(name, response_json[name])\n",
    "        result_object = response_json[name]\n",
    "        color = random_color()\n",
    "\n",
    "        if 'values' in result_object.keys():\n",
    "            block_lst = result_object['values']\n",
    "\n",
    "            for block in block_lst:\n",
    "                bounding_box = block.get('boundingBox', None)\n",
    "                confidence = block.get('confidence', 1)\n",
    "\n",
    "                if bounding_box and confidence > 0.8:\n",
    "                    x, y, w, h = bounding_box['x'], bounding_box['y'], bounding_box['w'], bounding_box['h']\n",
    "                    draw.rectangle([(x, y), (x + w, y + h)], outline=color, width=5)\n",
    "                    draw.text((x + 10, y + 10), text=feature, fill=color, font=font)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    # def upload_image(image_path):\n",
    "    #     print('upload image', image_path)\n",
    "    \n",
    "    # def input_image(image_path):\n",
    "    #     print('image image', image_path)\n",
    "\n",
    "    def change_image(image_path, features, language, smart_crops):\n",
    "        # print('change image', image_path)\n",
    "        if image_path:\n",
    "            response_json = request_vision(features, image_path, language, smart_crops)\n",
    "            \n",
    "            # print(response_json)\n",
    "            # for feature in features:\n",
    "            #     print(response_json['{}Result'.format(feature)])\n",
    "            \n",
    "            image = draw_image(image_path, features, response_json)\n",
    "            return image, response_json\n",
    "        else:\n",
    "            return None, None\n",
    "    \n",
    "    def change_features(features):\n",
    "        # print(features)\n",
    "        # features 를 받아서 value로 세팅\n",
    "        if 'smartCrops' in features:\n",
    "            return features, gr.update(visible=True)\n",
    "        else:\n",
    "            return features, gr.update(visible=False)\n",
    "    \n",
    "    def change_language(language, features):\n",
    "        print(language)\n",
    "        print(features, FEATURES[:4])\n",
    "        print(set(features), features)\n",
    "        # features 를 받아서 value로 세팅\n",
    "        value = list(set(FEATURES[:4]) & set(features))\n",
    "        if 'ko' == language:\n",
    "            return language, gr.update(choices=FEATURES[:4], value=value)\n",
    "        else:\n",
    "            return language, gr.update(choices=FEATURES, value=value)\n",
    "\n",
    "    # 언어선택, features, smartCrops 값\n",
    "    with gr.Column():\n",
    "        language_radio = gr.Radio(label='언어 선택', choices=['en', 'ko'], value='en')\n",
    "        features_checkbox = gr.CheckboxGroup(label='Features', choices=FEATURES)\n",
    "        smart_crops_textbox = gr.Textbox(label='Smart Crops')\n",
    "\n",
    "    # 입력 이미지\n",
    "    with gr.Column():\n",
    "        input_image = gr.Image(label='입력 이미지', type='filepath', height=400)\n",
    "        send_button = gr.Button('전송')\n",
    "\n",
    "    # 출력 이미지, JSON 형태\n",
    "    with gr.Row():\n",
    "        output_image = gr.Image(label='출력 이미지', type='pil', interactive=False)\n",
    "        output_json = gr.JSON(label='결과 JSON')\n",
    "    \n",
    "    # 이벤트\n",
    "    # input_image.input(input_image, inputs=[input_image], outputs=[])\n",
    "    # input_image.upload(upload_image, inputs=[input_image], outputs=[])\n",
    "    input_image.change(change_image, inputs=[input_image, features_checkbox, language_radio, smart_crops_textbox], outputs=[output_image, output_json])\n",
    "    send_button.click(change_image, inputs=[input_image, features_checkbox, language_radio, smart_crops_textbox], outputs=[output_image, output_json])\n",
    "    language_radio.change(change_language, inputs=[language_radio, features_checkbox], outputs=[language_radio, features_checkbox])\n",
    "    features_checkbox.change(change_features, inputs=[features_checkbox], outputs=[features_checkbox, smart_crops_textbox])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
